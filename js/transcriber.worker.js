import{pipeline,env}from"https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1";try{env.backends?.webgpu?(env.backends.webgpu.enabled=!0,console.log("WebGPU backend enabled for transformers.js")):env.backends?.onnx?(env.backends.onnx.webgpu??={},env.backends.onnx.webgpu.enabled=!0,console.log("WebGPU backend enabled for ONNX in transformers.js")):console.warn("No WebGPU or ONNX backend available, using CPU backend.")}catch(e){}let transcriber=null,currentModel=null;async function loadModel(e="Xenova/whisper-tiny.en"){transcriber&&currentModel===e||(transcriber=await pipeline("automatic-speech-recognition",e,{progress_callback:e=>{let s="download"===e.status?e.loaded/e.size*100:e.progress||0;self.postMessage({type:"progress",text:`Loading model: ${e.file}`,pct:s})}}),currentModel=e,self.postMessage({type:"ready"}))}async function doTranscribe(e,s,n){await loadModel(s);const a=new Float32Array(e),r={return_timestamps:"word",chunk_length_s:30,progress_callback:e=>{if(void 0!==e.progress){const s=95*e.progress;self.postMessage({type:"progress",text:`Transcribing... ${s.toFixed(2)}%`,pct:s})}}};n&&(r.task="translate");const t=await transcriber(a,r);self.postMessage({type:"result",output:t})}self.onmessage=async e=>{const{command:s,buffer:n,model:a,translate:r}=e.data;try{"init"===s?await loadModel(a):"transcribe"===s&&await doTranscribe(n,a,r)}catch(e){self.postMessage({type:"error",error:e.message})}};