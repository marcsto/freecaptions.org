import{pipeline,env}from"https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1";try{env.backends?.webgpu?(env.backends.webgpu.enabled=!0,console.log("WebGPU backend enabled for transformers.js")):env.backends?.onnx?(env.backends.onnx.webgpu??={},env.backends.onnx.webgpu.enabled=!0,console.log("WebGPU backend enabled for ONNX in transformers.js")):console.warn("No WebGPU or ONNX backend available, using CPU backend.")}catch(e){}let transcriber=null;async function loadModel(){transcriber||(transcriber=await pipeline("automatic-speech-recognition","Xenova/whisper-tiny.en",{progress_callback:e=>{let s="download"===e.status?e.loaded/e.size*100:e.progress||0;self.postMessage({type:"progress",text:`Loading model: ${e.file}`,pct:s})}}),self.postMessage({type:"ready"}))}async function doTranscribe(e){await loadModel();const s=new Float32Array(e),n=await transcriber(s,{return_timestamps:"word",chunk_length_s:30,progress_callback:e=>{if(void 0!==e.progress){const s=95*e.progress;self.postMessage({type:"progress",text:`Transcribing... ${s.toFixed(2)}%`,pct:s})}}});self.postMessage({type:"result",output:n})}self.onmessage=async e=>{const{command:s,buffer:n}=e.data;try{"init"===s?await loadModel():"transcribe"===s&&await doTranscribe(n)}catch(e){self.postMessage({type:"error",error:e.message})}};